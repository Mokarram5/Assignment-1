{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82be742c-8965-4d11-99a9-37250d50a552",
   "metadata": {},
   "source": [
    "## Assignment - Boosting-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5496d-0965-4323-aacf-0c2b36eea933",
   "metadata": {},
   "source": [
    "#### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc2ffa-ff56-4233-96f4-1164b8b22bbd",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce32a7-621d-4f57-a3af-74ef1ae48e23",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning algorithm that belongs to the ensemble learning family. It is a powerful and flexible technique used for both regression and classification tasks. The regression variant is often referred to as Gradient Boosting for Regression or Gradient Boosted Regression Trees (GBRT). Here's a brief overview:\r\n",
    "\r\n",
    "### Gradient Boosting Regression:\r\n",
    "\r\n",
    "1. **Base Learner:**\r\n",
    "   - The algorithm starts with an initial prediction, usually the mean of the target variable for regression problems.\r\n",
    "\r\n",
    "2. **Sequential Weak Learners:**\r\n",
    "   - Weak learners (typically decision trees) are added sequentially to the ensemble. Each tree is trained to correct the errors made by the combination of existing trees.\r\n",
    "\r\n",
    "3. **Gradient Descent Optimization:**\r\n",
    "   - At each iteration, the algorithm uses gradient descent optimization to minimize a loss function, which measures the difference between the current predictions and the true target values.\r\n",
    "\r\n",
    "4. **Learning Rate:**\r\n",
    "   - A learning rate parameter controls the step size during optimization. Smaller learning rates often lead to better generalization, but they require more iterations.\r\n",
    "\r\n",
    "5. **Predictions:**\r\n",
    "   - The final prediction is the sum of the predictions from all weak learners, each multiplied by a shrinkage factor (learning rate). The ensemble gradually refines its predictions by combining the strengths of multiple weak models.\r\n",
    "\r\n",
    "### Key Characteristics:\r\n",
    "\r\n",
    "- **Sequential Correction:**\r\n",
    "  - Each weak learner corrects the errors made by the ensemble so far, focusing on the instances that are challenging to predict.\r\n",
    "\r\n",
    "- **Flexible Loss Functions:**\r\n",
    "  - Gradient Boosting can accommodate various loss functions, making it adaptable to different types of regression problems.\r\n",
    "\r\n",
    "- **Regularization:**\r\n",
    "  - The algorithm can incorporate regularization techniques to control overfitting, such as limiting tree depth and adjusting learning rates.\r\n",
    "\r\n",
    "- **Robust to Outliers:**\r\n",
    "  - Gradient Boosting is generally robust to outliers due to its sequential learning process.\r\n",
    "\r\n",
    "### Implementation:\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "# Example with scikit-learn\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)\r\n",
    "\r\n",
    "# Initialize the Gradient Boosting Regressor\r\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\r\n",
    "\r\n",
    "# Train the model\r\n",
    "gb_regressor.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Make predictions\r\n",
    "predictions = gb_regressor.predict(X_test)\r\n",
    "\r\n",
    "# Evaluate the model\r\n",
    "mse = mean_squared_error(y_test, predictions)\r\n",
    "print(f\"Mean Squared Error: {mse}\")\r\n",
    "```\r\n",
    "\r\n",
    "Gradient Boosting Regression is widely used for tasks where accurate predictions are crucial, and it often performs well on structured tabular data. It has become a popular choice in machine learning competitions and real-world applications.and real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee033f0c-42e7-4576-8416-6fb2b2c32921",
   "metadata": {},
   "source": [
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca1c70-18a1-4fc4-bf8c-f67a171cda48",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e908e6-ff0a-4b0a-bb24-d82e48774505",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import mean_squared_error, r2_score\r\n",
    "\r\n",
    "# Generate a synthetic dataset\r\n",
    "np.random.seed(42)\r\n",
    "X = np.random.rand(100, 1)\r\n",
    "y = 5 * X.squeeze() + np.random.normal(scale=0.5, size=100)\r\n",
    "\r\n",
    "# Split the data into training and testing sets\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "# Define the Decision Tree as the weak learner\r\n",
    "class DecisionTree:\r\n",
    "    def __init__(self):\r\n",
    "        self.threshold = None\r\n",
    "        self.value = None\r\n",
    "        self.left = None\r\n",
    "        self.right = None\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        # Simple decision tree: find the threshold that minimizes the mean squared error\r\n",
    "        thresholds = np.unique(X)\r\n",
    "        best_mse = float('inf')\r\n",
    "        for threshold in thresholds:\r\n",
    "            left_mask = X <= threshold\r\n",
    "            right_mask = X > threshold\r\n",
    "            mse = np.mean((y[left_mask] - np.mean(y[left_mask]))**2) + \\\r\n",
    "                  np.mean((y[right_mask] - np.mean(y[right_mask]))**2)\r\n",
    "            if mse < best_mse:\r\n",
    "                best_mse = mse\r\n",
    "                self.threshold = threshold\r\n",
    "                self.value = np.mean(y)\r\n",
    "                self.left = DecisionTree()\r\n",
    "                self.right = DecisionTree()\r\n",
    "                self.left.fit(X[left_mask], y[left_mask])\r\n",
    "                self.right.fit(X[right_mask], y[right_mask])\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        if X <= self.threshold:\r\n",
    "            return self.left.predict(X) if self.left else self.value\r\n",
    "        else:\r\n",
    "            return self.right.predict(X) if self.right else self.value\r\n",
    "\r\n",
    "# Define the Gradient Boosting Regressor\r\n",
    "class GradientBoostingRegressor:\r\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\r\n",
    "        self.n_estimators = n_estimators\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.trees = []\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        # Initialize the model with the mean of the target variable\r\n",
    "        initial_prediction = np.mean(y)\r\n",
    "        residuals = y - initial_prediction\r\n",
    "\r\n",
    "        for _ in range(self.n_estimators):\r\n",
    "            tree = DecisionTree()\r\n",
    "            tree.fit(X, residuals)\r\n",
    "            prediction = [tree.predict(xi) for xi in X]\r\n",
    "            self.trees.append((self.learning_rate, tree))\r\n",
    "            residuals -= self.learning_rate * prediction\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        predictions = [learning_rate * tree.predict(X) for learning_rate, tree in self.trees]\r\n",
    "        return np.mean(predictions)\r\n",
    "\r\n",
    "# Train the Gradient Boosting Regressor\r\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\r\n",
    "gb_regressor.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Make predictions on the test set\r\n",
    "y_pred = [gb_regressor.predict(xi) for xi in X_test]\r\n",
    "\r\n",
    "# Evaluate the model\r\n",
    "mse = mean_squared_error(y_test, y_pred)\r\n",
    "r2 = r2_score(y_t```est, y_pred)\r\n",
    "\r\n",
    "print(f\"Mean Squared Error: {mse}\")\r\n",
    "print(f\"R-squared: {r2}\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13ed53-bf8d-4109-847c-e623f56e4785",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparametersrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f9f52-ad3f-449f-ace6-a7ee7fedb9bf",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f242e-a187-418b-8e50-658a3e6c7171",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "# Define the parameter grid\r\n",
    "param_grid = {\r\n",
    "    'n_estimators': [50, 100, 150],\r\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\r\n",
    "    'max_depth': [3, 5, 7]\r\n",
    "}\r\n",
    "\r\n",
    "# Create the Gradient Boosting Regressor\r\n",
    "gb_regressor = GradientBoostingRegressor()\r\n",
    "\r\n",
    "# Perform Grid Search\r\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Print the best hyperparameters\r\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\r\n",
    "\r\n",
    "# Make predictions on the test set using the best model\r\n",
    "best_gb_regressor = grid_search.best_estimator_\r\n",
    "y_pred = best_gb_regressor.predict(X_test)\r\n",
    "\r\n",
    "# Evaluate the model\r\n",
    "mse = mean_squared_error(y_test, y_pred)\r\n",
    "r2 = r2_score(y_test, y_pred)\r\n",
    "\r\n",
    "print(f\"Mean Squared Error: {mse}\")```\r\n",
    "print(f\"R-squared: {r2}\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd702b9e-6b20-4d96-9e08-51dbfe63bb97",
   "metadata": {},
   "source": [
    "#### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbe9fe-197d-4a04-8082-03bebfcc99e1",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef1132-9535-48a1-a458-18ad4ea33d45",
   "metadata": {},
   "source": [
    "In the context of gradient boosting, a weak learner is a model that performs slightly better than random guessing on a binary classification problem or predicts values that are only slightly better than the average on a regression problem. The key characteristic of a weak learner is that it is better than random chance, but it doesn't need to be a highly accurate or complex model.\r\n",
    "\r\n",
    "Gradient boosting builds an ensemble of weak learners, typically decision trees, in a sequential manner. Each weak learner is trained to correct the errors made by the combination of existing learners in the ensemble. The concept of using weak learners in boosting algorithms was introduced by Robert Schapire in the context of AdaBoost.\r\n",
    "\r\n",
    "Characteristics of a Weak Learner:\r\n",
    "\r\n",
    "1. **Slightly Better than Random Guessing:**\r\n",
    "   - A weak learner should have an accuracy or predictive performance that is only slightly better than random guessing. For binary classification, it should have an error rate less than 0.5.\r\n",
    "\r\n",
    "2. **Low Complexity:**\r\n",
    "   - Weak learners are often simple models, such as shallow decision trees (stumps) with a limited number of nodes or rules. The goal is to have models that capture only the most obvious patterns in the data.\r\n",
    "\r\n",
    "3. **Prone to Errors:**\r\n",
    "   - Since the purpose of a weak learner is to focus on instances that are difficult to classify or predict, it is allowed to make errors. In fact, a weak learner that is too accurate might dominate the ensemble and hinder the boosting process.\r\n",
    "\r\n",
    "4. **Adaptable:**\r\n",
    "   - Weak learners are trained sequentially, and each one adapts to the errors made by the previous learners. This adaptability is crucial for the boosting algorithm to iteratively improve its performance.\r\n",
    "\r\n",
    "5. **Efficient Training:**\r\n",
    "   - The training of weak learners should be computationally efficient, as the boosting algorithm builds the ensemble through multiple iterations.\r\n",
    "\r\n",
    "Common Weak Learners in Gradient Boosting:\r\n",
    "\r\n",
    "- **Decision Trees (Stumps):**\r\n",
    "  - Shallow decision trees with a limited number of nodes or rules are commonly used as weak learners.\r\n",
    "\r\n",
    "- **Linear Models:**\r\n",
    "  - Linear models, such as linear regression for regression tasks or logistic regression for classification tasks, can also serve as weak learners.\r\n",
    "\r\n",
    "- **SVM Classifiers:**\r\n",
    "  - Support Vector Machines with simple kernels can be used as weak learners in certain cases.\r\n",
    "\r\n",
    "The strength of gradient boosting lies in the combination of multiple weak learners, each specializing in different aspects of the data, to create a strong ensemble model that generalizes well to new, unseen instances.of the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97f15a-7a69-4ff1-a4ba-3fe9bc3aef96",
   "metadata": {},
   "source": [
    "#### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a186ca6-2981-4325-b7ce-4cddc4d658b0",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b68e7-19fa-4a5e-81d6-73a078f25f8f",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the following key concepts:\r\n",
    "\r\n",
    "1. **Ensemble Learning:**\r\n",
    "   - Gradient Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong predictive model.\r\n",
    "\r\n",
    "2. **Sequential Improvement:**\r\n",
    "   - Weak learners are added to the ensemble sequentially, and each new learner focuses on correcting the errors made by the existing ensemble. This sequential nature allows the algorithm to adapt and improve iteratively.\r\n",
    "\r\n",
    "3. **Gradient Descent Optimization:**\r\n",
    "   - Gradient Boosting is built on the principles of gradient descent optimization. At each iteration, the algorithm minimizes a loss function by adjusting the parameters of the weak learner in the direction of the negative gradient of the loss.\r\n",
    "\r\n",
    "4. **Residuals as Targets:**\r\n",
    "   - Each weak learner is trained to predict the residuals (the differences between the actual and predicted values) of the ensemble formed by the existing learners. This focuses the new learner on the instances where the current model performs poorly.\r\n",
    "\r\n",
    "5. **Learning Rate:**\r\n",
    "   - A learning rate parameter is introduced to control the step size during optimization. It scales the contribution of each weak learner to the ensemble. Smaller learning rates lead to more gradual updates, providing better generalization.\r\n",
    "\r\n",
    "6. **Combining Weak Predictions:**\r\n",
    "   - The final prediction is the sum of the predictions from all weak learners, each multiplied by a shrinkage factor (learning rate). The ensemble gradually refines its predictions by combining the strengths of multiple weak models.\r\n",
    "\r\n",
    "7. **Robustness to Overfitting:**\r\n",
    "   - Gradient Boosting is inherently resistant to overfitting due to the combination of weak models and the sequential training process, which focuses on the most challenging instances.\r\n",
    "\r\n",
    "8. **Flexibility:**\r\n",
    "   - Gradient Boosting is versatile and can be applied to various types of machine learning problems, including regression and classification. It can handle both numerical and categorical features.\r\n",
    "\r\n",
    "9. **Feature Importance:**\r\n",
    "   - The algorithm naturally provides information about feature importance. Features that consistently contribute more to reducing the loss are deemed more important in the final model.\r\n",
    "\r\n",
    "In summary, the intuition behind Gradient Boosting involves building a strong predictive model by iteratively improving weak learners. The algorithm focuses on instances where the current ensemble performs poorly and adapts to the data's patterns through a process of sequential optimization. This makes Gradient Boosting a powerful and widely used technique in machine learning, known for its high predictive accuracy and robustness.ques are commonly used for parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf2957-edde-4b11-89ad-9229c258fdd9",
   "metadata": {},
   "source": [
    "#### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fa790-e4e1-4b06-a901-c8e4131debb3",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f66fb1-c564-4733-8986-b03fda096d34",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially. Here is a step-by-step explanation of how this process occurs:\r\n",
    "\r\n",
    "1. **Initialization:**\r\n",
    "   - The algorithm starts with an initial prediction, often the mean (for regression problems) or the log-odds (for classification problems) of the target variable. This initial prediction serves as the starting point for building the ensemble.\r\n",
    "\r\n",
    "2. **Compute Residuals:**\r\n",
    "   - At each iteration, the residuals (the differences between the actual and predicted values) are computed. The weak learner added in the current iteration will focus on correcting these residuals.\r\n",
    "\r\n",
    "3. **Train Weak Learner:**\r\n",
    "   - A weak learner, typically a decision tree with limited depth (a stump) or a few nodes, is trained to predict the residuals. The goal is to fit the weak learner to the parts of the data where the current ensemble is performing poorly.\r\n",
    "\r\n",
    "4. **Compute Learning Rate Adjusted Predictions:**\r\n",
    "   - The predictions of the weak learner are multiplied by a learning rate, a hyperparameter that controls the step size during optimization. This scaling ensures that each weak learner contributes a fraction of its prediction to the final ensemble.\r\n",
    "\r\n",
    "5. **Update Ensemble:**\r\n",
    "   - The predictions from the current weak learner, adjusted by the learning rate, are added to the predictions of the existing ensemble. This update process is similar to gradient descent optimization.\r\n",
    "\r\n",
    "6. **Repeat:**\r\n",
    "   - Steps 2 to 5 are repeated for a predefined number of iterations (number of trees or weak learners). Each new weak learner focuses on correcting the errors made by the combination of the existing ensemble.\r\n",
    "\r\n",
    "7. **Final Prediction:**\r\n",
    "   - The final prediction is the sum of the predictions from all weak learners. The ensemble has learned to approximate the true relationship between the features and the target variable by iteratively improving its predictions.\r\n",
    "\r\n",
    "The key idea is that each new weak learner corrects the errors of the existing ensemble, gradually improving the overall predictive performance. The learning rate controls the impact of each weak learner, and the process continues until a specified number of iterations are completed.\r\n",
    "\r\n",
    "This sequential building of an ensemble allows Gradient Boosting to create a strong predictive model that captures complex patterns in the data, and the algorithm is known for its high accuracy and robustness.and accurate ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b067b0-a1af-4947-a73e-3419e654118e",
   "metadata": {},
   "source": [
    "#### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bdb33a-ca62-4363-b753-a8dcdcce5d3a",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf91443-cdab-4db7-9705-ecb59754a15f",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key components and steps in the optimization process. Here are the steps involved in constructing the mathematical intuition of Gradient Boosting:\r\n",
    "\r\n",
    "1. **Objective Function:**\r\n",
    "   - Define an objective function that quantifies the difference between the current predictions and the true target values. For regression, this objective function is often the mean squared error, and for classification, it could be the negative log-likelihood (deviance).\r\n",
    "\r\n",
    "2. **Initialization:**\r\n",
    "   - Initialize the model with an initial prediction. In the case of regression, it could be the mean of the target variable, and for classification, it could be the log-odds or probability of each class.\r\n",
    "\r\n",
    "3. **Compute Residuals:**\r\n",
    "   - Compute the residuals by taking the difference between the true target values and the current predictions. The residuals represent the errors made by the current model.\r\n",
    "\r\n",
    "4. **Train Weak Learner:**\r\n",
    "   - Train a weak learner (e.g., decision tree) to predict the residuals. The weak learner is fit to the residuals to capture the patterns in the data that the current model is not handling well.\r\n",
    "\r\n",
    "5. **Compute Learning Rate Adjusted Predictions:**\r\n",
    "   - Multiply the predictions of the weak learner by a learning rate. The learning rate is a hyperparameter that controls the step size during optimization. This scaling ensures that each weak learner contributes a fraction of its prediction to the final ensemble.\r\n",
    "\r\n",
    "6. **Update Ensemble:**\r\n",
    "   - Update the ensemble by adding the learning rate adjusted predictions to the current predictions. This step is analogous to performing a gradient descent step to minimize the objective function.\r\n",
    "\r\n",
    "7. **Repeat:**\r\n",
    "   - Repeat steps 3 to 6 for a specified number of iterations. At each iteration, a new weak learner is trained to predict the residuals, and its learning rate adjusted predictions are added to the ensemble.\r\n",
    "\r\n",
    "8. **Final Prediction:**\r\n",
    "   - The final prediction is the sum of the predictions from all weak learners. The ensemble has iteratively improved its predictions by focusing on the challenging instances in the data.\r\n",
    "\r\n",
    "9. **Regularization (Optional):**\r\n",
    "   - Optionally, introduce regularization terms to control the complexity of the weak learners and prevent overfitting. This can include constraints on the depth of decision trees or the inclusion of shrinkage terms.\r\n",
    "\r\n",
    "10. **Feature Importance:**\r\n",
    "    - Assess the importance of features based on their contribution to reducing the objective function. Features that consistently contribute more to the reduction in the loss are deemed more important.\r\n",
    "\r\n",
    "These steps collectively form the mathematical intuition behind Gradient Boosting. The algorithm aims to minimize the objective function by sequentially adding weak learners that focus on the instances where the current model performs poorly. The learning rate controls the influence of each weak learner, and the ensemble gradually refines its predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

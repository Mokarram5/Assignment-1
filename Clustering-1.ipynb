{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0263b0-78fe-4508-8983-fa5a3cefdaea",
   "metadata": {},
   "source": [
    "## Assignment - Clustering-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5496d-0965-4323-aacf-0c2b36eea933",
   "metadata": {},
   "source": [
    "#### Q1. What are the different types of clustering algorithms, and how do they differ in terms of their approach and underlying assumptions??isation libraries or tools as necessary...?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc2ffa-ff56-4233-96f4-1164b8b22bbd",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ed3c0-9e02-4827-867a-15f1ca26c923",
   "metadata": {},
   "source": [
    "Clustering algorithms are techniques used in unsupervised learning to group similar data points together. Here are some common types of clustering algorithms and their key characteristics:\r\n",
    "\r\n",
    "1. **K-Means Clustering:**\r\n",
    "   - **Approach:** Partition the data into K clusters, where each data point belongs to the cluster with the nearest mean.\r\n",
    "   - **Assumptions:** Assumes clusters are spherical and equally sized. Works well when clusters are approximately isotropic (similar shapes) and have similar sizes.\r\n",
    "\r\n",
    "2. **Hierarchical Clustering:**\r\n",
    "   - **Approach:** Builds a hierarchy of clusters either bottom-up (agglomerative) or top-down (divisive) by merging or splitting clusters based on a distance metric.\r\n",
    "   - **Assumptions:** Does not assume a specific number of clusters. Captures hierarchical relationships in the data.\r\n",
    "\r\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\r\n",
    "   - **Approach:** Clusters dense regions of data points and marks low-density regions as noise.\r\n",
    "   - **Assumptions:** Assumes clusters are areas of high density separated by areas of lower density. Effective for data with irregular shapes and varying cluster sizes.\r\n",
    "\r\n",
    "4. **Mean Shift:**\r\n",
    "   - **Approach:** Non-parametric clustering method that shifts centroids toward the mode of the data distribution.\r\n",
    "   - **Assumptions:** Does not assume a specific number of clusters. Adapts to the shape of the data distribution.\r\n",
    "\r\n",
    "5. **Agglomerative Clustering:**\r\n",
    "   - **Approach:** Hierarchical clustering method that starts with individual data points and merges the closest ones until a single cluster is formed.\r\n",
    "   - **Assumptions:** Does not assume a specific number of clusters. The choice of linkage (e.g., ward, average, complete) influences the shape of clusters.\r\n",
    "\r\n",
    "6. **Gaussian Mixture Models (GMM):**\r\n",
    "   - **Approach:** Assumes that the data is generated by a mixture of several Gaussian distributions and uses an Expectation-Maximization (EM) algorithm to fit the model.\r\n",
    "   - **Assumptions:** Assumes that the data points are generated from a mixture of Gaussian distributions. More flexible than K-Means for capturing elliptical clusters.\r\n",
    "\r\n",
    "7. **Self-Organizing Maps (SOM):**\r\n",
    "   - **Approach:** Neural network-based method that organizes data into a grid of neurons, preserving the topology of the input space.\r\n",
    "   - **Assumptions:** Often used for visualizing high-dimensional data. Captures the intrinsic structure of the input space.\r\n",
    "\r\n",
    "Each clustering algorithm has its strengths and weaknesses, and the choice depends on the characteristics of the data and the goals of the analysis. Understanding the assumptions and approaches of different algorithms is crucial for selecting the most suitable method for a particular dataset.s (PCA) and spectral analysis. lower-dimensional space.ning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dbf6c-e444-44eb-b194-ca251361552d",
   "metadata": {},
   "source": [
    "#### Q2.What is K-means clustering, and how does it work?."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca1c70-18a1-4fc4-bf8c-f67a171cda48",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be23b8-5200-401c-8c21-be7875e1926c",
   "metadata": {},
   "source": [
    "K-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping subsets (clusters). The goal of the K-means algorithm is to group similar data points together while minimizing the within-cluster sum of squares.\r\n",
    "\r\n",
    "Here's a step-by-step explanation of how K-means clustering works:\r\n",
    "\r\n",
    "1. **Initialization:**\r\n",
    "   - Choose the number of clusters (K) you want to identify in the dataset.\r\n",
    "   - Randomly initialize K cluster centroids. These centroids serve as the initial centers of the clusters.\r\n",
    "\r\n",
    "2. **Assignment Step:**\r\n",
    "   - For each data point, calculate the distance to each centroid.\r\n",
    "   - Assign the data point to the cluster whose centroid is the closest (typically using Euclidean distance).\r\n",
    "\r\n",
    "3. **Update Step:**\r\n",
    "   - Recalculate the centroids of the clusters based on the mean of the data points assigned to each cluster.\r\n",
    "\r\n",
    "4. **Repeat:**\r\n",
    "   - Repeat the assignment and update steps until convergence criteria are met. Convergence occurs when the centroids no longer change significantly, or the assignments stabilize.\r\n",
    "\r\n",
    "The algorithm minimizes the within-cluster sum of squares, which is the sum of the squared distances between each data point and the centroid of its assigned cluster. This process continues until the clusters stabilize, and data points remain relatively consistent within their assigned clusters.\r\n",
    "\r\n",
    "**Key Points:**\r\n",
    "- The algorithm converges to a local minimum of the within-cluster sum of squares, and different initializations may lead to different final results.\r\n",
    "- K-means assumes that clusters are spherical and equally sized, making it sensitive to outliers and elongated clusters.\r\n",
    "- The choice of K (number of clusters) is a crucial parameter that may require exploration and validation through techniques like the elbow method or silhouette analysis.\r\n",
    "\r\n",
    "Despite its assumptions and limitations, K-means is widely used due to its simplicity, efficiency, and effectiveness for various clustering tasks.atical and scientific domains.d for dimensionality reduction. techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c6d72-44a0-45dc-8c0e-7f65cc38a9e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Q3. What are some advantages and limitations of K-means clustering compared to other clustering techniques??."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f9f52-ad3f-449f-ace6-a7ee7fedb9bf",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e1fb3-2b0e-44a7-974a-879d2e0194a3",
   "metadata": {},
   "source": [
    "**Advantages of K-Means Clustering:**\r\n",
    "\r\n",
    "1. **Simplicity and Speed:**\r\n",
    "   - K-means is computationally efficient and scales well with large datasets.\r\n",
    "   - The simplicity of the algorithm makes it easy to implement and interpret.\r\n",
    "\r\n",
    "2. **Scalability:**\r\n",
    "   - Handles a large number of variables efficiently.\r\n",
    "   - Suitable for datasets with a relatively large number of features.\r\n",
    "\r\n",
    "3. **Ease of Interpretation:**\r\n",
    "   - Results in clearly defined and non-overlapping clusters.\r\n",
    "   - Intuitive and straightforward interpretation of results.\r\n",
    "\r\n",
    "4. **Versatility:**\r\n",
    "   - Applicable to various types of data, including numerical and categorical variables.\r\n",
    "   - Can be adapted for use in a wide range of domains.\r\n",
    "\r\n",
    "5. **Convergence:**\r\n",
    "   - K-means usually converges to a solution, providing a stable result.\r\n",
    "\r\n",
    "**Limitations of K-Means Clustering:**\r\n",
    "\r\n",
    "1. **Assumption of Spherical Clusters:**\r\n",
    "   - K-means assumes that clusters are spherical, equally sized, and isotropic, making it less suitable for non-uniformly shaped or elongated clusters.\r\n",
    "\r\n",
    "2. **Sensitivity to Initialization:**\r\n",
    "   - Results can be sensitive to the initial placement of centroids, leading to different final clusters with different initializations.\r\n",
    "\r\n",
    "3. **Difficulty with Varying Cluster Sizes:**\r\n",
    "   - Struggles with clusters of different sizes and densities as it tries to create equally sized clusters.\r\n",
    "\r\n",
    "4. **Outlier Sensitivity:**\r\n",
    "   - Sensitive to outliers as they can significantly affect the position of centroids and, consequently, the cluster assignments.\r\n",
    "\r\n",
    "5. **Fixed Number of Clusters (K):**\r\n",
    "   - Requires specifying the number of clusters (K) beforehand, which may not always be known or easily determined.\r\n",
    "\r\n",
    "6. **Not Suitable for Complex Geometries:**\r\n",
    "   - May perform poorly when dealing with clusters with complex geometries or non-linear structures.\r\n",
    "\r\n",
    "7. **Limited Applicability to Non-Numeric Data:**\r\n",
    "   - Primarily designed for numeric data and may not perform well on datasets with categorical variables.\r\n",
    "\r\n",
    "8. **Assumption of Homogeneous Variance:**\r\n",
    "   - Assumes that clusters have similar variances, which may not hold in real-world scenarios.\r\n",
    "\r\n",
    "While K-means is a popular and widely used clustering algorithm, it's essential to consider these limitations and choose an appropriate clustering technique based on the characteristics of the data and the specific goals of the analysis. Other clustering algorithms like hierarchical clustering, DBSCAN, or Gaussian Mixture Models (GMM) may be more suitable in certain situations.the eigen-decomposition form.aximum variance, respectively.ng techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b2e4e-94ae-421d-9867-b42f5cf8e2bd",
   "metadata": {},
   "source": [
    "#### Q4. How do you determine the optimal number of clusters in K-means clustering, and what are some common methods for doing so??."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbe9fe-197d-4a04-8082-03bebfcc99e1",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165144fc-347d-4fd8-82dd-453708269f8e",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters (K) in K-means clustering is a crucial step to ensure meaningful and useful results. Several methods can be employed to find the optimal K, and here are some common ones:\r\n",
    "\r\n",
    "1. **Elbow Method:**\r\n",
    "   - Plot the within-cluster sum of squares (WCSS) against the number of clusters.\r\n",
    "   - Look for the \"elbow\" point, where the rate of decrease in WCSS starts to slow down.\r\n",
    "   - The point at which further increasing the number of clusters provides diminishing returns is often considered the optimal K.\r\n",
    "\r\n",
    "2. **Silhouette Analysis:**\r\n",
    "   - Calculate the silhouette score for different values of K.\r\n",
    "   - The silhouette score measures how similar an object is to its own cluster compared to other clusters.\r\n",
    "   - Choose the K with the highest silhouette score, indicating well-defined clusters.\r\n",
    "\r\n",
    "3. **Gap Statistics:**\r\n",
    "   - Compare the performance of the clustering algorithm on the actual data with its performance on randomly generated data (with no inherent clustering).\r\n",
    "   - The optimal K is the one that maximizes the gap between the within-cluster sum of squares of the actual data and the average within-cluster sum of squares of the random data.\r\n",
    "\r\n",
    "4. **Davies-Bouldin Index:**\r\n",
    "   - Measures the compactness and separation of clusters.\r\n",
    "   - Lower Davies-Bouldin Index values indicate better clustering, so the optimal K minimizes this index.\r\n",
    "\r\n",
    "5. **Cross-Validation:**\r\n",
    "   - Split the dataset into training and testing sets.\r\n",
    "   - Perform K-means clustering on the training set for different values of K.\r\n",
    "   - Evaluate the clustering performance on the testing set.\r\n",
    "   - Choose the K that maximizes performance on the testing set.\r\n",
    "\r\n",
    "6. **Gap Statistic:**\r\n",
    "   - Compare the within-cluster sum of squares of the original data with a reference distribution obtained from randomly generated data.\r\n",
    "   - Optimal K is the one that maximizes the gap between the two.\r\n",
    "\r\n",
    "7. **Calinski-Harabasz Index:**\r\n",
    "   - Measures the ratio of between-cluster variance to within-cluster variance.\r\n",
    "   - Higher values indicate better-defined clusters, so the optimal K maximizes this index.\r\n",
    "\r\n",
    "It's important to note that different methods may suggest different values of K, and there is no one-size-fits-all solution. The choice of the optimal K often involves a combination of these methods and domain-specific knowledge. It's advisable to visualize the clusters and assess the practical relevance of the identified groups in addition to using quantitative measures.the study of symmetric linear operators.lysis or modeling task.chine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97f15a-7a69-4ff1-a4ba-3fe9bc3aef96",
   "metadata": {},
   "source": [
    "#### Q5. What are some applications of K-means clustering in real-world scenarios, and how has it been used to solve specific problems??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a186ca6-2981-4325-b7ce-4cddc4d658b0",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08cebc-6d25-4be3-852e-797376330a78",
   "metadata": {},
   "source": [
    "K-means clustering has found application in various real-world scenarios due to its simplicity, efficiency, and versatility. Here are some applications where K-means clustering has been used to solve specific problems:\r\n",
    "\r\n",
    "1. **Customer Segmentation:**\r\n",
    "   - Identifying distinct groups of customers based on their purchasing behavior, preferences, and demographics.\r\n",
    "   - Helps businesses tailor marketing strategies, product recommendations, and customer service for different segments.\r\n",
    "\r\n",
    "2. **Image Compression:**\r\n",
    "   - Reducing the number of colors in an image by clustering similar pixel colors together.\r\n",
    "   - Improves storage efficiency and speeds up image processing.\r\n",
    "\r\n",
    "3. **Anomaly Detection:**\r\n",
    "   - Identifying unusual patterns or outliers in data that deviate from the norm.\r\n",
    "   - Used in cybersecurity to detect anomalous network activities, fraud detection in finance, or quality control in manufacturing.\r\n",
    "\r\n",
    "4. **Document Clustering:**\r\n",
    "   - Organizing large sets of documents into meaningful clusters based on content similarity.\r\n",
    "   - Enables better document organization, topic modeling, and information retrieval.\r\n",
    "\r\n",
    "5. **Genetic Data Analysis:**\r\n",
    "   - Grouping individuals based on genetic similarities to identify population structures or genetic disorders.\r\n",
    "   - Aids in genetic research and personalized medicine.\r\n",
    "\r\n",
    "6. **Retail Inventory Management:**\r\n",
    "   - Grouping products based on sales patterns and demand, helping retailers optimize inventory levels.\r\n",
    "   - Improves stock management and reduces overstock or stockouts.\r\n",
    "\r\n",
    "7. **Social Network Analysis:**\r\n",
    "   - Identifying communities or groups within a social network based on user interactions.\r\n",
    "   - Enhances targeted advertising, recommendation systems, and understanding user behavior.\r\n",
    "\r\n",
    "8. **Remote Sensing and Image Segmentation:**\r\n",
    "   - Segmentation of satellite or aerial images into meaningful regions.\r\n",
    "   - Used in agriculture for crop monitoring, land cover classification, and urban planning.\r\n",
    "\r\n",
    "9. **Healthcare:**\r\n",
    "   - Clustering patient data to identify similar medical profiles for personalized treatment plans.\r\n",
    "   - Used in disease diagnosis, treatment optimization, and healthcare resource allocation.\r\n",
    "\r\n",
    "10. **Text Document Classification:**\r\n",
    "    - Grouping similar documents or articles based on their content.\r\n",
    "    - Enhances information retrieval, topic modeling, and organization of large document repositories.\r\n",
    "\r\n",
    "11. **Traffic Flow Analysis:**\r\n",
    "    - Clustering road segments based on traffic patterns to optimize traffic management.\r\n",
    "    - Helps in designing efficient transportation systems and reducing congestion.\r\n",
    "\r\n",
    "K-means clustering, with its simplicity and efficiency, continues to be a valuable tool in various domains for pattern recognition, segmentation, and decision-making based on similarity.nd solving differential equations.e analysis or modeling task.uction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf2957-edde-4b11-89ad-9229c258fdd9",
   "metadata": {},
   "source": [
    "#### Q6. How do you interpret the output of a K-means clustering algorithm, and what insights can you derive from the resulting clusters???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fa790-e4e1-4b06-a901-c8e4131debb3",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c290a-7e8b-4425-9c1f-f351c34c5514",
   "metadata": {},
   "source": [
    "Interpreting the output of a K-means clustering algorithm involves understanding the characteristics of each cluster and extracting meaningful insights from the grouping. Here are key steps and insights derived from the resulting clusters:\r\n",
    "\r\n",
    "1. **Cluster Centers (Centroids):**\r\n",
    "   - The coordinates of the cluster centers represent the average feature values of data points in each cluster.\r\n",
    "   - Analyze the centroid values to understand the typical profile of each cluster.\r\n",
    "\r\n",
    "2. **Within-Cluster Sum of Squares (WCSS):**\r\n",
    "   - WCSS measures the compactness of clusters. Lower WCSS indicates tighter and more well-defined clusters.\r\n",
    "   - Evaluate WCSS for different values of K and look for an \"elbow\" in the plot to identify an optimal number of clusters.\r\n",
    "\r\n",
    "3. **Data Distribution Within Clusters:**\r\n",
    "   - Examine the distribution of data points within each cluster to understand its shape and spread.\r\n",
    "   - Visualize clusters to identify if they are compact or elongated, dense or sparse.\r\n",
    "\r\n",
    "4. **Silhouette Score:**\r\n",
    "   - Silhouette score measures how well-separated clusters are and ranges from -1 to 1.\r\n",
    "   - Positive scores indicate good separation, while negative scores suggest overlapping clusters.\r\n",
    "\r\n",
    "5. **Interpretation of Features:**\r\n",
    "   - Identify which features contribute the most to the differences between clusters.\r\n",
    "   - Features with high variance or significant differences between cluster centroids are crucial for interpretation.\r\n",
    "\r\n",
    "6. **Comparing Cluster Profiles:**\r\n",
    "   - If applicable, compare cluster profiles with external variables or labels to validate the quality of clustering.\r\n",
    "   - Understand the relevance of clusters in the context of the problem domain.\r\n",
    "\r\n",
    "7. **Domain-Specific Analysis:**\r\n",
    "   - Use domain knowledge to interpret the clusters. Understand why certain data points are grouped together and what characteristics define each group.\r\n",
    "   - Consider the practical implications of the clusters.\r\n",
    "\r\n",
    "8. **Pattern Recognition:**\r\n",
    "   - Identify patterns or trends within and across clusters.\r\n",
    "   - Explore whether clusters represent distinct subgroups or variations within the dataset.\r\n",
    "\r\n",
    "9. **Iterative Refinement:**\r\n",
    "   - If the initial clustering does not provide meaningful insights, consider refining the process.\r\n",
    "   - Adjust hyperparameters, preprocess data differently, or try different clustering algorithms.\r\n",
    "\r\n",
    "10. **Business Impact:**\r\n",
    "    - Relate the clusters to business or research objectives. Understand how the insights gained can inform decision-making or strategy.\r\n",
    "    - Evaluate the practical significance and potential actions based on the identified clusters.\r\n",
    "\r\n",
    "Interpreting the output of a K-means clustering algorithm requires a combination of statistical analysis, visualization, and domain expertise. It's essential to approach the interpretation with a clear understanding of the problem context and the objectives of the clustering analysis.directions of variation in data.ious data analysis and modeling tasks., unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4516a0-dda7-4f0d-87c4-b5422416fecb",
   "metadata": {},
   "source": [
    "#### Q7. What are some common challenges in implementing K-means clustering, and how can you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6224786-34a8-4594-885f-a8763844b755",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa087a4-11ae-458c-8eee-88f4cb3a02bf",
   "metadata": {},
   "source": [
    "Implementing K-means clustering can encounter several challenges, and it's important to be aware of these issues to obtain reliable and meaningful results. Here are some common challenges and strategies to address them:\r\n",
    "\r\n",
    "1. **Sensitivity to Initial Centroid Positions:**\r\n",
    "   - Challenge: K-means is sensitive to the initial placement of centroids, which may lead to different final clusters.\r\n",
    "   - Solution: Use techniques like k-means++ initialization, which intelligently selects initial centroids to improve convergence.\r\n",
    "\r\n",
    "2. **Choosing the Optimal Number of Clusters (K):**\r\n",
    "   - Challenge: Determining the optimal value of K is often subjective and may impact the quality of clustering.\r\n",
    "   - Solution: Employ methods such as the elbow method, silhouette analysis, or cross-validation to find an appropriate K value.\r\n",
    "\r\n",
    "3. **Handling Outliers:**\r\n",
    "   - Challenge: K-means is sensitive to outliers, which can distort cluster centroids and affect the overall clustering.\r\n",
    "   - Solution: Consider using robust versions of K-means or preprocessing data to identify and handle outliers before clustering.\r\n",
    "\r\n",
    "4. **Non-Spherical Clusters:**\r\n",
    "   - Challenge: K-means assumes spherical clusters, making it less effective for non-spherical or irregularly shaped clusters.\r\n",
    "   - Solution: Explore alternative clustering algorithms such as DBSCAN or hierarchical clustering, which can handle non-spherical shapes.\r\n",
    "\r\n",
    "5. **Scaling and Standardization:**\r\n",
    "   - Challenge: Features with different scales can disproportionately influence the clustering process.\r\n",
    "   - Solution: Standardize or normalize features to ensure that all variables contribute equally to the distance calculations.\r\n",
    "\r\n",
    "6. **Interpretability and Validation:**\r\n",
    "   - Challenge: Interpreting the clusters and validating their meaningfulness can be subjective.\r\n",
    "   - Solution: Use domain knowledge to interpret clusters and employ validation metrics like silhouette score or domain-specific criteria.\r\n",
    "\r\n",
    "7. **Computational Complexity:**\r\n",
    "   - Challenge: K-means can become computationally expensive for large datasets or a high number of dimensions.\r\n",
    "   - Solution: Consider using variants of K-means optimized for large datasets or employ dimensionality reduction techniques like PCA.\r\n",
    "\r\n",
    "8. **Handling Categorical Data:**\r\n",
    "   - Challenge: K-means is designed for numerical data and may not handle categorical variables well.\r\n",
    "   - Solution: Convert categorical variables into numerical representations (e.g., one-hot encoding) or explore clustering algorithms suitable for categorical data.\r\n",
    "\r\n",
    "9. **Addressing Skewed Distributions:**\r\n",
    "   - Challenge: Skewed or imbalanced data distributions can impact the performance of K-means.\r\n",
    "   - Solution: Explore preprocessing techniques to address skewness, such as log-transformations or using clustering algorithms robust to skewed data.\r\n",
    "\r\n",
    "10. **Stability of Clusters:**\r\n",
    "    - Challenge: Clusters may change with different runs, impacting the stability of the results.\r\n",
    "    - Solution: Run the algorithm multiple times with different initializations and evaluate the consistency of the obtained clusters.\r\n",
    "\r\n",
    "Addressing these challenges requires a combination of thoughtful preprocessing, parameter tuning, and careful interpretation of results. It's also important to consider alternative clustering algorithms when K-means may not be the most suitable choice for specific scenarios.ts in the feature space.r of dimensions to retain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

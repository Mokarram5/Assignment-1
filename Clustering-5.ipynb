{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0263b0-78fe-4508-8983-fa5a3cefdaea",
   "metadata": {},
   "source": [
    "## Assignment - Clustering-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff2195-221f-402e-a87f-a5cca1df6ee0",
   "metadata": {},
   "source": [
    "#### Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc2ffa-ff56-4233-96f4-1164b8b22bbd",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ed3c0-9e02-4827-867a-15f1ca26c923",
   "metadata": {},
   "source": [
    "A contingency matrix, also known as a confusion matrix or error matrix, is a table that describes the performance of a classification model on a set of test data for which the true values are known. It is a tool used for evaluating the performance of a classification algorithm by comparing predicted and actual class labels.\r\n",
    "\r\n",
    "The contingency matrix has four entries:\r\n",
    "\r\n",
    "1. **True Positive (TP):** The number of instances that were correctly predicted as positive.\r\n",
    "\r\n",
    "2. **False Positive (FP):** The number of instances that were predicted as positive but actually belong to the negative class.\r\n",
    "\r\n",
    "3. **True Negative (TN):** The number of instances that were correctly predicted as negative.\r\n",
    "\r\n",
    "4. **False Negative (FN):** The number of instances that were predicted as negative but actually belong to the positive class.\r\n",
    "\r\n",
    "The layout of a contingency matrix is as follows:\r\n",
    "\r\n",
    "\\[\r\n",
    "\\begin{matrix}\r\n",
    "TN & FP \\\\\r\n",
    "FN & TP \\\\\r\n",
    "\\end{matrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "From the contingency matrix, various performance metrics can be derived, including:\r\n",
    "\r\n",
    "- **Accuracy:** \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\r\n",
    "- **Precision (Positive Predictive Value):** \\(\\frac{TP}{TP + FP}\\)\r\n",
    "- **Recall (Sensitivity or True Positive Rate):** \\(\\frac{TP}{TP + FN}\\)\r\n",
    "- **Specificity (True Negative Rate):** \\(\\frac{TN}{TN + FP}\\)\r\n",
    "- **F1 Score:** \\(\\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\r\n",
    "\r\n",
    "These metrics provide insights into different aspects of the classification model's performance, such as its ability to correctly identify positive instances (precision), its ability to capture all positive instances (recall), and the trade-off between precision and recall (F1 score).\r\n",
    "\r\n",
    "In summary, a contingency matrix is a fundamental tool for assessing the performance of a classification model, allowing a more detailed analysis of its predictive capabilities beyond a single accuracy score.ering performance.s and patterns within complex datasets.r of clusters is unknown or not predetermined.thod for a particular dataset.s (PCA) and spectral analysis. lower-dimensional space.ning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4fbd0-ef0f-42a1-97d5-67f632c5086b",
   "metadata": {},
   "source": [
    "#### Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca1c70-18a1-4fc4-bf8c-f67a171cda48",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7d610-edbd-4988-aad5-f0ddca2ab535",
   "metadata": {},
   "source": [
    "A pair confusion matrix is a variant of the regular confusion matrix that specifically focuses on the pairwise classification of instances belonging to two classes. While a regular confusion matrix provides an overall view of the performance across all classes, a pair confusion matrix narrows its focus to the binary classification of two specific classes.\r\n",
    "\r\n",
    "The layout of a pair confusion matrix is as follows:\r\n",
    "\r\n",
    "\\[\r\n",
    "\\begin{matrix}\r\n",
    "TN & FP \\\\\r\n",
    "FN & TP \\\\\r\n",
    "\\end{matrix}\r\n",
    "\\]\r\n",
    "\r\n",
    "where:\r\n",
    "- \\(TN\\) (True Negative): Instances correctly predicted as the negative class.\r\n",
    "- \\(FP\\) (False Positive): Instances predicted as the positive class but actually belong to the negative class.\r\n",
    "- \\(FN\\) (False Negative): Instances predicted as the negative class but actually belong to the positive class.\r\n",
    "- \\(TP\\) (True Positive): Instances correctly predicted as the positive class.\r\n",
    "\r\n",
    "**Differences from a Regular Confusion Matrix:**\r\n",
    "1. **Focus on Two Classes:** A pair confusion matrix specifically focuses on the performance of a classifier for a binary classification problem involving two classes, whereas a regular confusion matrix accommodates multiple classes.\r\n",
    "\r\n",
    "2. **Simplicity:** The pair confusion matrix simplifies the analysis for binary classification, making it easier to interpret and extract relevant performance metrics.\r\n",
    "\r\n",
    "**Usefulness in Certain Situations:**\r\n",
    "- **Imbalanced Classes:** In situations where there is a significant class imbalance, a pair confusion matrix can provide a more detailed understanding of how well the classifier is performing on the minority class, which might be of greater interest.\r\n",
    "\r\n",
    "- **Specific Class Pair Analysis:** In some applications, the focus may be on the performance of the classifier for a specific pair of classes due to their relevance or significance in the context of the problem.\r\n",
    "\r\n",
    "- **Binary Evaluation Metrics:** Pair confusion matrices are particularly useful when calculating binary evaluation metrics such as precision, recall, specificity, and the F1 score.\r\n",
    "\r\n",
    "In summary, a pair confusion matrix is a specialized tool that enhances the analysis of binary classification performance, offering a more detailed perspective, especially in scenarios where the emphasis is on specific class pairs or imbalanced class distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0d108-935b-4d4c-9137-ef28fc9c47e0",
   "metadata": {},
   "source": [
    "#### Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f9f52-ad3f-449f-ace6-a7ee7fedb9bf",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f2f5e-7ae1-4a55-b5e8-175f56a2b601",
   "metadata": {},
   "source": [
    "In the context of natural language processing (NLP), an extrinsic measure refers to an evaluation metric or criterion that assesses the performance of a language model based on its effectiveness in completing a specific downstream task. Unlike intrinsic measures that evaluate a model's performance based on its capabilities within the scope of a particular training or intermediate task, extrinsic measures focus on the model's utility in real-world applications or practical scenarios.\r\n",
    "\r\n",
    "**Key Characteristics of Extrinsic Measures in NLP:**\r\n",
    "\r\n",
    "1. **Task-Specific Evaluation:** Extrinsic measures involve evaluating a language model within the context of a specific application or task, such as sentiment analysis, named entity recognition, machine translation, or question answering.\r\n",
    "\r\n",
    "2. **Real-World Relevance:** The goal of extrinsic measures is to determine how well a language model generalizes its learned knowledge to real-world scenarios. This relevance is crucial for assessing the model's practical utility.\r\n",
    "\r\n",
    "3. **End-to-End Evaluation:** Extrinsic evaluation typically involves assessing the model's performance on the entire pipeline or workflow of a given task, from input processing to generating the final output.\r\n",
    "\r\n",
    "4. **User-Centric Metrics:** Extrinsic measures often include metrics that are meaningful from a user's perspective. For example, in machine translation, metrics like BLEU score or human evaluation may be considered extrinsic measures.\r\n",
    "\r\n",
    "**Example:**\r\n",
    "\r\n",
    "Consider a scenario where a language model is trained for sentiment analysis. In an intrinsic evaluation, one might assess the model's ability to predict sentiment polarity on a labeled sentiment dataset. In contrast, an extrinsic evaluation would involve integrating the sentiment model into an application, such as a social media monitoring tool, and assessing its performance based on how well it supports the overall goal of sentiment analysis in a real-world setting.\r\n",
    "\r\n",
    "**Usage:**\r\n",
    "\r\n",
    "Extrinsic measures are crucial for understanding how well a language model meets the end goals of NLP applications. They help researchers and practitioners gauge the practical effectiveness and usability of models in real-world contexts, guiding decisions about model deployment and usage.\r\n",
    "\r\n",
    "Common extrinsic evaluation practices involve:\r\n",
    "\r\n",
    "- **Application-Specific Metrics:** Metrics tailored to the specific requirements of the downstream task, such as accuracy, precision, recall, F1 score, or task-specific metrics like BLEU for machine translation.\r\n",
    "\r\n",
    "- **User Studies:** Direct involvement of users or human evaluators to assess the model's performance and user satisfaction in practical applications.\r\n",
    "\r\n",
    "In summary, extrinsic measures provide a more comprehensive and application-focused assessment of language models in real-world scenarios, contributing to the understanding of their practical impact and usefulness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4b693-d5f5-485d-8264-27833bdce1ba",
   "metadata": {},
   "source": [
    "#### Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbe9fe-197d-4a04-8082-03bebfcc99e1",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65e8a9-411d-4061-8a29-87630d9ba2fe",
   "metadata": {},
   "source": [
    "In the context of machine learning, intrinsic and extrinsic measures are two types of evaluation methods used to assess the performance of models. Let's explore the differences between intrinsic and extrinsic measures:\r\n",
    "\r\n",
    "1. **Intrinsic Measures:**\r\n",
    "   - **Definition:** Intrinsic measures evaluate a model's performance based on its capabilities within the scope of a specific training or intermediate task.\r\n",
    "   - **Characteristics:** They are task-specific and focus on assessing the model's performance on a particular aspect of its functionality.\r\n",
    "   - **Use Case:** Intrinsic measures are commonly employed during the development and training phases to understand how well a model is learning and adapting to the specific training objectives.\r\n",
    "   - **Examples:**\r\n",
    "     - For a language model, perplexity or cross-entropy loss during language modeling training is an intrinsic measure.\r\n",
    "     - In computer vision, intrinsic measures could include accuracy, loss, or other task-specific metrics during image classification training.\r\n",
    "\r\n",
    "2. **Extrinsic Measures:**\r\n",
    "   - **Definition:** Extrinsic measures evaluate a model's performance based on its effectiveness in completing a downstream or real-world task.\r\n",
    "   - **Characteristics:** They are task-agnostic and focus on assessing the model's utility in practical scenarios or applications.\r\n",
    "   - **Use Case:** Extrinsic measures are used to determine how well a model generalizes its learned knowledge to real-world tasks and applications.\r\n",
    "   - **Examples:**\r\n",
    "     - For a language model, an extrinsic measure might involve assessing its performance in a sentiment analysis application or machine translation task.\r\n",
    "     - In computer vision, an extrinsic measure could involve evaluating the model's performance in an object detection or image captioning application.\r\n",
    "\r\n",
    "**Key Differences:**\r\n",
    "- **Scope:** Intrinsic measures assess specific aspects of a model's performance during training, while extrinsic measures evaluate the overall utility of the model in real-world applications.\r\n",
    "  \r\n",
    "- **Task Dependency:** Intrinsic measures are closely tied to the specific task for which the model is trained, while extrinsic measures are task-agnostic and focus on broader applications.\r\n",
    "\r\n",
    "- **Development vs. Deployment:** Intrinsic measures are often used during the model development and training phases, while extrinsic measures play a crucial role in deciding whether a model is suitable for deployment in real-world scenarios.\r\n",
    "\r\n",
    "**Usage:**\r\n",
    "- **Intrinsic Measures:** Useful for model development, debugging, and understanding how well the model learns the training data.\r\n",
    "  \r\n",
    "- **Extrinsic Measures:** Crucial for assessing the practical impact and usability of a model in real-world applications, guiding decisions about deployment and usage.\r\n",
    "\r\n",
    "In summary, while intrinsic measures provide insights into how well a model learns during training, extrinsic measures are essential for understanding a model's practical effectiveness and utility in real-world contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441eb631-04bf-4595-8c2c-5448556f0836",
   "metadata": {},
   "source": [
    "#### Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a186ca6-2981-4325-b7ce-4cddc4d658b0",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08cebc-6d25-4be3-852e-797376330a78",
   "metadata": {},
   "source": [
    "A confusion matrix is a fundamental tool in machine learning for evaluating the performance of a classification model. It is a table that summarizes the results of a classification problem, breaking down the predictions made by the model into four categories: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Here's a brief explanation of these terms:\r\n",
    "\r\n",
    "- **True Positives (TP):** Instances that are actually positive (belong to the positive class) and are correctly predicted as positive by the model.\r\n",
    "\r\n",
    "- **True Negatives (TN):** Instances that are actually negative (belong to the negative class) and are correctly predicted as negative by the model.\r\n",
    "\r\n",
    "- **False Positives (FP):** Instances that are actually negative but are incorrectly predicted as positive by the model.\r\n",
    "\r\n",
    "- **False Negatives (FN):** Instances that are actually positive but are incorrectly predicted as negative by the model.\r\n",
    "\r\n",
    "The confusion matrix is organized as follows:\r\n",
    "\r\n",
    "```\r\n",
    "                 | Predicted Positive | Predicted Negative |\r\n",
    "-----------------|--------------------|--------------------|\r\n",
    "Actual Positive  |        TP          |        FN          |\r\n",
    "Actual Negative  |        FP          |        TN          |\r\n",
    "```\r\n",
    "\r\n",
    "**Purpose of Confusion Matrix:**\r\n",
    "\r\n",
    "1. **Performance Evaluation:** It provides a detailed breakdown of how well a model performs in terms of correct and incorrect predictions.\r\n",
    "\r\n",
    "2. **Metrics Calculation:** Based on the values in the confusion matrix, various performance metrics can be calculated, such as accuracy, precision, recall, F1 score, and specificity.\r\n",
    "\r\n",
    "3. **Model Improvement:** By analyzing the confusion matrix, one can identify areas of improvement for the model, such as reducing false positives or false negatives.\r\n",
    "\r\n",
    "4. **Class Imbalances:** It helps in situations where there is an imbalance in the distribution of classes, allowing a more nuanced evaluation beyond overall accuracy.\r\n",
    "\r\n",
    "**Using Confusion Matrix to Identify Strengths and Weaknesses:**\r\n",
    "\r\n",
    "- **Accuracy:** The overall correctness of the model's predictions is measured by accuracy. It is calculated as (TP + TN) / (TP + FP + TN + FN).\r\n",
    "\r\n",
    "- **Precision:** Precision is the ratio of correctly predicted positive observations to the total predicted positives (TP / (TP + FP)). It measures how many of the predicted positive instances are actually positive.\r\n",
    "\r\n",
    "- **Recall (Sensitivity):** Recall is the ratio of correctly predicted positive observations to the total actual positives (TP / (TP + FN)). It measures how many of the actual positive instances are correctly predicted.\r\n",
    "\r\n",
    "- **F1 Score:** F1 score is the harmonic mean of precision and recall, providing a balance between the two. It is calculated as 2 * (Precision * Recall) / (Precision + Recall).\r\n",
    "\r\n",
    "Analyzing these metrics from the confusion matrix helps in understanding the strengths and weaknesses of a model. For example, a high precision indicates that the model has fewer false positives, while a high recall indicates that the model captures a large portion of actual positives.\r\n",
    "\r\n",
    "In summary, the confusion matrix is a powerful tool for assessing the performance of classification models, and its analysis allows for a comprehensive understanding of the strengths and weaknesses of the model across different performance aspects.ering performance. clusters is known in advance.subsequent analyses.tion, and decision-making based on similarity.nd solving differential equations.e analysis or modeling task.uction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a7ce3-f9a8-4a74-bf98-15e0ba211311",
   "metadata": {},
   "source": [
    "#### Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fa790-e4e1-4b06-a901-c8e4131debb3",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c290a-7e8b-4425-9c1f-f351c34c5514",
   "metadata": {},
   "source": [
    "In the context of unsupervised learning algorithms, intrinsic evaluation measures are used to assess the performance of models without relying on external labels or ground truth. These measures are particularly important in scenarios where the true classes or categories of the data are unknown. Here are some common intrinsic measures used in unsupervised learning:\r\n",
    "\r\n",
    "1. **Silhouette Score:**\r\n",
    "   - **Interpretation:** The silhouette score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). It ranges from -1 to 1, where a higher score indicates better-defined clusters.\r\n",
    "   - **Usage:** Higher silhouette scores suggest well-separated and compact clusters.\r\n",
    "\r\n",
    "2. **Davies-Bouldin Index:**\r\n",
    "   - **Interpretation:** The Davies-Bouldin index quantifies the compactness and separation between clusters. A lower index indicates better clustering, with lower intra-cluster distances and higher inter-cluster distances.\r\n",
    "   - **Usage:** Minimizing the Davies-Bouldin index leads to more optimal clustering.\r\n",
    "\r\n",
    "3. **Calinski-Harabasz Index (Variance Ratio Criterion):**\r\n",
    "   - **Interpretation:** The Calinski-Harabasz index measures the ratio of the between-cluster variance to the within-cluster variance. Higher values indicate better-defined clusters.\r\n",
    "   - **Usage:** Maximizing the Calinski-Harabasz index is desirable for effective clustering.\r\n",
    "\r\n",
    "4. **Inertia (Within-Cluster Sum of Squares):**\r\n",
    "   - **Interpretation:** Inertia measures the sum of squared distances of samples to their closest cluster center. Lower inertia suggests more compact clusters.\r\n",
    "   - **Usage:** Minimizing inertia is a goal in clustering to obtain tighter clusters.\r\n",
    "\r\n",
    "5. **Dunn Index:**\r\n",
    "   - **Interpretation:** The Dunn index assesses the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. Higher Dunn index values indicate better-defined clusters.\r\n",
    "   - **Usage:** Maximizing the Dunn index leads to better clustering results.\r\n",
    "\r\n",
    "6. **Gap Statistic:**\r\n",
    "   - **Interpretation:** The gap statistic compares the within-cluster dispersion of the data to that of a reference random dataset. A larger gap indicates more distinct clustering.\r\n",
    "   - **Usage:** Comparing the gap statistic across different clusterings helps in determining the optimal number of clusters.\r\n",
    "\r\n",
    "7. **Adjusted Rand Index (ARI) for Clustering:**\r\n",
    "   - **Interpretation:** ARI measures the similarity between true and predicted clusterings while correcting for chance. It ranges from -1 to 1, with higher values indicating better agreement.\r\n",
    "   - **Usage:** ARI is useful when true cluster labels are available for some evaluation.\r\n",
    "\r\n",
    "Interpreting these intrinsic measures involves considering the specific characteristics of the dataset and the goals of the clustering task. It's essential to choose a measure that aligns with the desired properties of the clusters, such as compactness, separation, and overall quality. Additionally, combining multiple measures can provide a more comprehensive evaluation of unsupervised learning algorithms.d problem context.paces may also be considered.ustering analysis.the objectives of the clustering analysis.directions of variation in data.ious data analysis and modeling tasks., unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3bd5c-6781-46fb-aba3-9168a5101870",
   "metadata": {},
   "source": [
    "#### Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6224786-34a8-4594-885f-a8763844b755",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa087a4-11ae-458c-8eee-88f4cb3a02bf",
   "metadata": {},
   "source": [
    "Using accuracy as the sole evaluation metric for classification tasks has some limitations, and it may not provide a complete picture of a model's performance. Here are some of the limitations and ways to address them:\r\n",
    "\r\n",
    "1. **Sensitivity to Class Imbalance:**\r\n",
    "   - **Limitation:** Accuracy can be misleading when classes are imbalanced. A classifier may achieve high accuracy by predicting the majority class while neglecting the minority class.\r\n",
    "   - **Addressing:** Use additional metrics such as precision, recall, F1-score, or area under the receiver operating characteristic (ROC) curve to evaluate performance across different classes. These metrics provide insights into the model's ability to handle imbalanced datasets.\r\n",
    "\r\n",
    "2. **Misleading in the Presence of Unequal Misclassification Costs:**\r\n",
    "   - **Limitation:** In scenarios where misclassifying one class is more costly than another, accuracy may not reflect the true impact of misclassifications.\r\n",
    "   - **Addressing:** Employ metrics like precision, recall, or the Matthews correlation coefficient (MCC) that consider false positives and false negatives separately. Cost-sensitive learning techniques can also be applied to address imbalanced costs.\r\n",
    "\r\n",
    "3. **Disregard for Class Probabilities:**\r\n",
    "   - **Limitation:** Accuracy does not consider the confidence or probability scores assigned by a classifier to its predictions.\r\n",
    "   - **Addressing:** Use metrics like log loss or area under the precision-recall curve (AUC-PR) that take into account the probability estimates of class memberships. These metrics provide a more nuanced evaluation, especially for models providing probability scores.\r\n",
    "\r\n",
    "4. **Insensitive to Type of Errors:**\r\n",
    "   - **Limitation:** Accuracy treats all misclassifications equally, regardless of whether they are false positives or false negatives.\r\n",
    "   - **Addressing:** Consider precision and recall, which focus on specific types of errors. Precision measures the accuracy of positive predictions, while recall assesses the ability to capture all positive instances.\r\n",
    "\r\n",
    "5. **Influence of Class Priors:**\r\n",
    "   - **Limitation:** Accuracy can be influenced by the distribution of classes, especially when class priors are imbalanced.\r\n",
    "   - **Addressing:** Use metrics like balanced accuracy, which accounts for imbalances in class sizes. Balanced accuracy is the average of recall scores for each class.\r\n",
    "\r\n",
    "6. **Threshold Sensitivity:**\r\n",
    "   - **Limitation:** Accuracy is sensitive to the choice of the classification threshold.\r\n",
    "   - **Addressing:** Analyze the precision-recall curve or receiver operating characteristic curve to understand the trade-offs between true positive rate and false positive rate across different thresholds.\r\n",
    "\r\n",
    "While accuracy remains a valuable metric, it should be complemented by a range of other metrics that provide a more nuanced evaluation of classification performance, especially in complex and imbalanced scenarios.ing of the clustering performance.structure of the data.etection techniques.st suitable choice for specific scenarios.ts in the feature space.r of dimensions to retain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c627e7dd-ea07-475f-953e-aea2e0bd79ca",
   "metadata": {},
   "source": [
    "## Assignment - Ensemble Techniques And Its Types-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2070c1-717a-40e0-85ab-7a7c50710efb",
   "metadata": {},
   "source": [
    "#### Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57578384-d44c-4a8f-b55f-51599be4790c",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2b076-57e1-4536-aab0-e4976880471c",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning refers to the combination of multiple individual models to create a more robust and accurate predictive model. The idea behind ensemble methods is to leverage the strengths of various base models while mitigating their individual weaknesses. By combining the predictions of multiple models, ensemble methods often outperform individual models and provide more reliable results.\r\n",
    "\r\n",
    "There are several types of ensemble techniques, with the two main categories being:\r\n",
    "\r\n",
    "1. **Bagging (Bootstrap Aggregating):**\r\n",
    "   - **Idea:** Multiple instances of the same learning algorithm are trained on different subsets of the training data.\r\n",
    "   - **Example Algorithm:** Random Forest, where multiple decision trees are trained on different random subsets of the training data, and their predictions are combined through voting (classification) or averaging (regression).\r\n",
    "\r\n",
    "2. **Boosting:**\r\n",
    "   - **Idea:** Weak learners (models that perform slightly better than random chance) are sequentially trained, with each new model giving more emphasis to the instances that the previous models struggled with.\r\n",
    "   - **Example Algorithm:** AdaBoost (Adaptive Boosting), where a series of weak learners (e.g., shallow decision trees) are trained, and each subsequent model focuses on the misclassified instances of the previous models.\r\n",
    "\r\n",
    "Ensemble techniques offer several advantages:\r\n",
    "\r\n",
    "- **Increased Accuracy:** Ensembles often provide better accuracy compared to individual models, especially when combining diverse models that capture different aspects of the data.\r\n",
    "\r\n",
    "- **Robustness:** Ensembles are more robust to overfitting and outliers, as the impact of individual errors tends to be mitigated when combined with predictions from other models.\r\n",
    "\r\n",
    "- **Improved Generalization:** Ensembles generalize well to new, unseen data, enhancing the model's ability to make accurate predictions on a broader range of instances.\r\n",
    "\r\n",
    "- **Versatility:** Ensemble methods can be applied to various types of base models, making them versatile and applicable to different machine learning problems.\r\n",
    "\r\n",
    "Common ensemble techniques include Random Forest, AdaBoost, Gradient Boosting, and Stacking. The choice of the ensemble method depends on the characteristics of the data and the specific problem at hand. is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039d588-50b7-48c8-8c1a-bfa53aedc298",
   "metadata": {},
   "source": [
    "#### Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25802504-c738-48e2-9b10-d05a451e8075",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05e141-792a-48a4-b3fd-8360f1838ec0",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons, and they offer various advantages that contribute to improved model performance and robustness. Here are some key reasons why ensemble techniques are widely used:\r\n",
    "\r\n",
    "1. **Increased Accuracy:**\r\n",
    "   - **Diverse Models:** Ensemble methods combine predictions from multiple models, often of different types or trained on different subsets of the data. This diversity helps capture different aspects of the underlying patterns in the data.\r\n",
    "   - **Reduction of Individual Errors:** By aggregating predictions, ensemble methods can mitigate the impact of errors made by individual models, leading to more accurate overall predictions.\r\n",
    "\r\n",
    "2. **Robustness:**\r\n",
    "   - **Mitigation of Overfitting:** Ensemble techniques are effective in reducing overfitting, especially when individual models overfit the training data. The combination of multiple models with different sources of error tends to result in a more robust and generalizable model.\r\n",
    "   - **Outlier Handling:** The impact of outliers or noisy instances can be minimized by combining predictions from multiple models, which may not be affected by outliers in the same way.\r\n",
    "\r\n",
    "3. **Improved Generalization:**\r\n",
    "   - **Enhanced Adaptability:** Ensembles often generalize well to new, unseen data. The collective knowledge of diverse models can lead to a more adaptable and accurate model when applied to instances outside the training set.\r\n",
    "   - **Reduced Sensitivity:** Ensemble methods are less sensitive to variations in the training data, making them suitable for datasets with diverse characteristics.\r\n",
    "\r\n",
    "4. **Versatility:**\r\n",
    "   - **Applicability to Various Models:** Ensemble techniques are applicable to a wide range of base models, including decision trees, linear models, support vector machines, and more. This versatility allows practitioners to leverage ensemble methods across different machine learning tasks.\r\n",
    "\r\n",
    "5. **Flexible Frameworks:**\r\n",
    "   - **Easy Implementation:** Many ensemble methods are relatively easy to implement and integrate into existing machine learning workflows. Libraries like scikit-learn provide built-in implementations of popular ensemble algorithms.\r\n",
    "   - **Parameter Tuning:** Ensembles offer flexibility in hyperparameter tuning, allowing practitioners to fine-tune the performance of the ensemble to suit the specific characteristics of the data.\r\n",
    "\r\n",
    "6. **Handling Imbalanced Data:**\r\n",
    "   - **Balancing Class Distribution:** Ensembles can be effective in handling imbalanced datasets, where one class is underrepresented. By combining models that address different aspects of the data, ensembles can improve predictions for minority classes.\r\n",
    "\r\n",
    "Common ensemble methods include Random Forest, AdaBoost, Gradient Boosting, and Stacking. The choice of a specific ensemble method depends on the nature of the data and the characteristics of the underlying problem.nts of your problem. inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221c46c-4c7a-44f0-a9cb-a64bd7c8f08a",
   "metadata": {},
   "source": [
    "#### Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4c286-ca90-4c5e-b8ac-3dd7b72cc207",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bac4c-27cb-4583-918f-7febc0be2ca0",
   "metadata": {},
   "source": [
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning where multiple instances of the same learning algorithm are trained on different subsets of the training data. The basic idea behind bagging is to reduce variance and improve the stability and accuracy of a model by combining predictions from multiple models trained on diverse subsets of the data.\r\n",
    "\r\n",
    "Here's how bagging works:\r\n",
    "\r\n",
    "1. **Bootstrap Sampling:**\r\n",
    "   - Random subsets of the training data are created by sampling with replacement (bootstrap sampling). This means that each subset can contain duplicate instances, and some instances may be left out.\r\n",
    "\r\n",
    "2. **Model Training:**\r\n",
    "   - Multiple instances of the same learning algorithm (base model) are trained independently on each of the bootstrap samples. Each instance sees a slightly different version of the training data.\r\n",
    "\r\n",
    "3. **Aggregation:**\r\n",
    "   - The predictions from each individual model are combined to form a final prediction. The aggregation process typically involves averaging predictions for regression problems or voting for classification problems.\r\n",
    "\r\n",
    "The key advantages of bagging include:\r\n",
    "\r\n",
    "- **Reducing Overfitting:** Bagging helps reduce overfitting by training each model on different subsets of the data, making the overall model more robust.\r\n",
    "\r\n",
    "- **Improving Stability:** By combining predictions from diverse models, bagging reduces the impact of individual model errors and outliers, leading to a more stable and reliable prediction.\r\n",
    "\r\n",
    "- **Handling Variability:** Bagging is effective when the underlying model is sensitive to variations in the training data. It smoothens the learning process and improves generalization.\r\n",
    "\r\n",
    "One of the most popular bagging algorithms is the **Random Forest**. In a Random Forest, the base models are decision trees, and each tree is trained on a different subset of the data. Additionally, at each split in a tree, a random subset of features is considered, adding an extra layer of randomness.\r\n",
    "\r\n",
    "In summary, bagging is a powerful ensemble technique that leverages the diversity of multiple models to improve predictive performance, reduce overfitting, and enhance the robustness of machine learning models.orithm.decisions.ed model complexity.m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506f36b-ec4b-4131-97bd-a2529494878f",
   "metadata": {},
   "source": [
    "#### Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e31682-51b7-4ce7-98f0-5190cceb24fd",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9709b-71f6-446f-a76b-3f440ae34024",
   "metadata": {},
   "source": [
    "Boosting is another ensemble technique in machine learning that combines multiple weak learners to create a strong learner. Unlike bagging, where models are trained independently on different subsets of the data, boosting involves training models sequentially, with each subsequent model giving more emphasis to instances that the previous models struggled with.\r\n",
    "\r\n",
    "Here's how boosting works:\r\n",
    "\r\n",
    "1. **Sequential Model Training:**\r\n",
    "   - A series of weak learners (models that perform slightly better than random chance) are trained sequentially.\r\n",
    "   - Each model is trained to correct the errors made by the previous models.\r\n",
    "\r\n",
    "2. **Instance Weighting:**\r\n",
    "   - Instances that were misclassified by the previous models are given higher weights, so the subsequent models focus more on getting these instances correct.\r\n",
    "\r\n",
    "3. **Combination of Models:**\r\n",
    "   - The final prediction is made by combining the predictions of all the weak learners, often using a weighted sum.\r\n",
    "\r\n",
    "Key characteristics and benefits of boosting include:\r\n",
    "\r\n",
    "- **Sequential Correction:** Boosting focuses on correcting the mistakes of the previous models, leading to improved overall performance.\r\n",
    "  \r\n",
    "- **Adaptive Learning:** The algorithm adapts its focus over iterations to give more attention to instances that are difficult to classify.\r\n",
    "\r\n",
    "- **Complexity:** Boosting can combine a collection of weak learners to create a strong, highly accurate model, even if the individual models are relatively simple.\r\n",
    "\r\n",
    "- **Applicability to Various Models:** Boosting can be applied to various base models, such as decision trees, linear models, or even neural networks.\r\n",
    "\r\n",
    "Popular boosting algorithms include:\r\n",
    "\r\n",
    "1. **AdaBoost (Adaptive Boosting):**\r\n",
    "   - AdaBoost assigns weights to misclassified instances, and subsequent models focus on correctly classifying those instances.\r\n",
    "   - Weak learners are typically shallow decision trees.\r\n",
    "\r\n",
    "2. **Gradient Boosting:**\r\n",
    "   - In gradient boosting, each model is trained to correct the residuals (errors) of the previous model.\r\n",
    "   - Common implementations include XGBoost, LightGBM, and CatBoost.\r\n",
    "\r\n",
    "3. **Stochastic Gradient Boosting:**\r\n",
    "   - Similar to gradient boosting, but it introduces randomness by subsampling instances and features to reduce overfitting.\r\n",
    "\r\n",
    "Boosting is powerful for improving model accuracy, especially in situations where simple models might struggle. However, it can be sensitive to noisy data or outliers, and care should be taken to tune hyperparameters appropriately. The choice of boosting algorithm depends on the specific characteristics of the data and the problem at hand.oblems.ke the SVM robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4b0aa-3542-43e7-9344-67e0dec5593b",
   "metadata": {},
   "source": [
    "#### Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cd730-89b7-4244-bc2f-d635a4a2e36e",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4308ba-ff6a-4263-9cfe-0af30d3c015d",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in machine learning, making them widely used and effective for a variety of tasks. Here are some key advantages of using ensemble techniques:\r\n",
    "\r\n",
    "1. **Increased Accuracy:**\r\n",
    "   - Ensemble methods often achieve higher accuracy compared to individual models. Combining predictions from multiple models helps mitigate the impact of errors made by individual models and leads to more accurate overall predictions.\r\n",
    "\r\n",
    "2. **Reduction of Overfitting:**\r\n",
    "   - Ensembles are less prone to overfitting, especially when the base models are diverse. Overfitting occurs when a model learns the noise in the training data, but the combination of multiple models tends to generalize better to new, unseen data.\r\n",
    "\r\n",
    "3. **Improved Robustness:**\r\n",
    "   - Ensembles are more robust to outliers or noisy instances in the data. The combination of predictions from different models can reduce the influence of outliers that may adversely affect individual models.\r\n",
    "\r\n",
    "4. **Better Generalization:**\r\n",
    "   - Ensemble methods enhance the generalization ability of models. By combining diverse models that capture different aspects of the underlying patterns, ensembles perform well on a broader range of instances and adapt well to new data.\r\n",
    "\r\n",
    "5. **Versatility:**\r\n",
    "   - Ensemble techniques are versatile and can be applied to various types of base models, making them suitable for different machine learning tasks and algorithms. They can be used with decision trees, linear models, support vector machines, and more.\r\n",
    "\r\n",
    "6. **Flexibility in Model Choice:**\r\n",
    "   - Ensemble methods allow practitioners to use different types of models as base learners. This flexibility enables the incorporation of both simple and complex models into the ensemble, depending on the characteristics of the data.\r\n",
    "\r\n",
    "7. **Handling Imbalanced Data:**\r\n",
    "   - Ensembles can effectively handle imbalanced datasets by combining models that address different aspects of the data. This is particularly useful for problems where one class is underrepresented.\r\n",
    "\r\n",
    "8. **Interpretability and Explainability:**\r\n",
    "   - In some cases, ensembles can provide insights into feature importance and model interpretability. Techniques like feature importance in Random Forests allow understanding the impact of each feature on the predictions.\r\n",
    "\r\n",
    "9. **Easy Implementation:**\r\n",
    "   - Many ensemble methods are easy to implement, and libraries like scikit-learn provide built-in implementations of popular ensemble algorithms. This ease of implementation facilitates the practical application of ensemble techniques.\r\n",
    "\r\n",
    "10. **State-of-the-Art Performance:**\r\n",
    "    - Ensembles, particularly those based on boosting algorithms like XGBoost and LightGBM, have consistently demonstrated state-of-the-art performance in various machine learning competitions and real-world applications.\r\n",
    "\r\n",
    "While ensemble techniques offer numerous advantages, it's essential to consider factors such as computational complexity, interpretability, and the potential for overfitting. The choice of the ensemble method depends on the characteristics of the data and the specific requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a287f8-e218-4fe7-80e5-26561923a865",
   "metadata": {},
   "source": [
    "#### Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac138da1-4d5a-4522-a980-491e17e2f633",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa819801-efbc-4bbc-8593-f0504561df11",
   "metadata": {},
   "source": [
    "While ensemble techniques generally offer several advantages and often lead to improved performance, they are not guaranteed to be better than individual models in all situations. The effectiveness of ensemble techniques depends on various factors, and there are scenarios where individual models might perform equally well or even outperform ensembles. Here are some considerations:\r\n",
    "\r\n",
    "1. **Diversity of Base Models:**\r\n",
    "   - The success of ensemble methods often hinges on the diversity of the base models. If the individual models in the ensemble are too similar or prone to the same types of errors, the benefits of ensemble learning may be limited.\r\n",
    "\r\n",
    "2. **Noise and Outliers:**\r\n",
    "   - Ensembles can be sensitive to noise and outliers in the data. If the dataset contains significant noise or outliers, individual models might make errors on these instances, and combining them in an ensemble may not always result in better predictions.\r\n",
    "\r\n",
    "3. **Computational Resources:**\r\n",
    "   - Ensembles can be computationally more demanding than individual models, especially when dealing with large datasets or complex algorithms. In situations where computational resources are limited, the overhead of running an ensemble may not be justified.\r\n",
    "\r\n",
    "4. **Interpretability:**\r\n",
    "   - Ensembles, particularly those with a large number of models, may be less interpretable than individual models. If interpretability is a crucial requirement, using a single, interpretable model might be preferred.\r\n",
    "\r\n",
    "5. **Overfitting:**\r\n",
    "   - While ensembles are less prone to overfitting, there can be cases where the ensemble itself overfits the training data, especially if the number of base models is excessively high or if the models are too complex. This is more likely to occur when the ensemble is not appropriately regularized.\r\n",
    "\r\n",
    "6. **Small Datasets:**\r\n",
    "   - In situations where the dataset is small, and there is limited diversity in the data, ensembles may not provide significant advantages. Individual models may perform well without the need for combining predictions.\r\n",
    "\r\n",
    "7. **Type of Problem:**\r\n",
    "   - The type of problem being addressed can influence the effectiveness of ensemble techniques. For some simpler problems, a well-tuned individual model might be sufficient, and the additional complexity of an ensemble may not be necessary.\r\n",
    "\r\n",
    "8. **Model Choice:**\r\n",
    "   - The choice of base models matters. If the individual models selected for the ensemble are not suitable for the problem at hand or are poorly trained, the ensemble's performance may not be better than that of a well-designed individual model.\r\n",
    "\r\n",
    "In practice, it's recommended to experiment with both individual models and ensemble methods, and the choice depends on the specific characteristics of the data and the goals of the machine learning task. Careful consideration of the factors mentioned above and empirical validation on the specific problem are crucial for determining whether ensemble techniques are the right choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce986a-c701-4a6d-a484-56890a4bf730",
   "metadata": {},
   "source": [
    "#### Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e3f14-c819-4c33-b8b4-d0b0ab642bea",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3b25e-e827-40bf-aec7-2691c737b906",
   "metadata": {},
   "source": [
    "The confidence interval calculated using the bootstrap method involves resampling from the observed data to estimate the sampling distribution of a statistic, and then using percentiles of this distribution to construct an interval. Here are the general steps to calculate a bootstrap confidence interval:\r\n",
    "\r\n",
    "1. **Data Resampling (Bootstrap Sampling):**\r\n",
    "   - Randomly sample, with replacement, from the observed dataset to create a \"bootstrap sample.\" This sample has the same size as the original dataset but may contain repeated instances and miss some original instances.\r\n",
    "\r\n",
    "2. **Statistic Calculation:**\r\n",
    "   - Calculate the statistic of interest (e.g., mean, median, standard deviation, etc.) on the bootstrap sample. This step is often done to mimic the process of estimating the parameter of interest from a different sample.\r\n",
    "\r\n",
    "3. **Repeat Resampling and Statistic Calculation:**\r\n",
    "   - Repeat steps 1 and 2 a large number of times (e.g., B times) to create a distribution of the statistic of interest, known as the \"bootstrap distribution.\"\r\n",
    "\r\n",
    "4. **Confidence Interval Calculation:**\r\n",
    "   - Use the percentiles of the bootstrap distribution to construct the confidence interval. Common choices include the percentiles corresponding to the desired confidence level (e.g., 95%).\r\n",
    "\r\n",
    "The confidence interval is typically constructed by taking percentiles from the bootstrap distribution. For a 95% confidence interval, you might use the 2.5th percentile as the lower bound and the 97.5th percentile as the upper bound. This interval contains the middle 95% of the bootstrap distribution, providing a range of plausible values for the parameter of interest.\r\n",
    "\r\n",
    "In formulaic terms, if `B` is the number of bootstrap samples and `theta_hat` is the observed estimate of the parameter:\r\n",
    "\r\n",
    "- Lower Bound: `theta_hat - quantile(bootstrap_distribution, alpha/2)`\r\n",
    "- Upper Bound: `theta_hat + quantile(bootstrap_distribution, 1 - alpha/2)`\r\n",
    "\r\n",
    "Here, `alpha` is the significance level (1 - confidence level), and the quantile function gives the value at a specific percentile in the distribution.\r\n",
    "\r\n",
    "Keep in mind that the bootstrap method assumes that the observed data is representative of the population, and the resampling procedure is used to approximate the distribution of the statistic in the absence of additional information about the population. The choice of the number of bootstrap samples (`B`) is an important consideration and depends on the specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37423f-b460-47aa-91fd-24808ae26563",
   "metadata": {},
   "source": [
    "#### Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d18179-9ce7-41a8-a9df-b0d865d02b52",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d52711-d8b8-4714-bb7d-ad4e748261c4",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic by repeatedly resampling from the observed data. It allows us to make inferences about the population based on the observed sample without assuming a specific parametric form for the population distribution. Here are the general steps involved in the bootstrap procedure:\r\n",
    "\r\n",
    "1. **Original Data:**\r\n",
    "   - Start with a dataset containing observed data. Let's denote this dataset as \\(X\\) with \\(n\\) observations.\r\n",
    "\r\n",
    "2. **Resampling (With Replacement):**\r\n",
    "   - Randomly draw \\(n\\) samples (with replacement) from the original dataset. This creates a bootstrap sample, denoted as \\(X^*_1\\).\r\n",
    "\r\n",
    "3. **Statistic Calculation:**\r\n",
    "   - Calculate the statistic of interest (e.g., mean, median, standard deviation, etc.) on the bootstrap sample \\(X^*_1\\). This statistic is denoted as \\(\\theta^*_1\\).\r\n",
    "\r\n",
    "4. **Repeat Steps 2 and 3:**\r\n",
    "   - Repeat steps 2 and 3 a large number of times (e.g., B times), each time creating a new bootstrap sample (\\(X^*_i\\)) and calculating the corresponding statistic (\\(\\theta^*_i\\)).\r\n",
    "\r\n",
    "5. **Bootstrap Distribution:**\r\n",
    "   - Collect all the calculated statistics \\(\\theta^*_i\\) to create the bootstrap distribution. This distribution represents the variability of the statistic under repeated sampling from the observed data.\r\n",
    "\r\n",
    "6. **Confidence Intervals:**\r\n",
    "   - Use the bootstrap distribution to construct confidence intervals for the statistic of interest. Commonly used percentiles (e.g., 2.5th and 97.5th percentiles for a 95% confidence interval) are used to define the interval.\r\n",
    "\r\n",
    "7. **Inference:**\r\n",
    "   - Make statistical inferences about the population parameter based on the characteristics of the bootstrap distribution. For example, one might estimate the standard error, bias, or other properties of the statistic.\r\n",
    "\r\n",
    "The key idea behind bootstrap is to simulate the process of drawing new samples from the population by repeatedly resampling from the observed sample. This process allows us to empirically estimate the sampling distribution of a statistic, even if the underlying population distribution is unknown or complex.\r\n",
    "\r\n",
    "Bootstrap is widely used for various purposes, including estimating standard errors, constructing confidence intervals, and assessing the variability and distributional properties of a statistic. It is particularly useful when analytical methods for obtaining the sampling distribution are challenging or not available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51025cf9-ab29-499c-9bb6-1a24cd5482d7",
   "metadata": {},
   "source": [
    "#### Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe6efa-d8b8-4890-9d91-4b7be38b16b0",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548f9a2-7162-435b-8440-0949243e2302",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap, we'll follow the steps mentioned earlier. In this case, we'll use the observed sample data to create bootstrap samples and calculate the mean height for each sample. Here are the steps:\r\n",
    "\r\n",
    "1. **Original Data:**\r\n",
    "   - The original data is the sample of 50 tree heights with a mean of 15 meters and a standard deviation of 2 meters.\r\n",
    "\r\n",
    "2. **Resampling (With Replacement):**\r\n",
    "   - Randomly draw 50 samples (with replacement) from the observed sample.\r\n",
    "\r\n",
    "3. **Statistic Calculation:**\r\n",
    "   - Calculate the mean height for each bootstrap sample.\r\n",
    "\r\n",
    "4. **Repeat Steps 2 and 3:**\r\n",
    "   - Repeat steps 2 and 3 a large number of times (e.g., B times), each time creating a new bootstrap sample and calculating the mean height.\r\n",
    "\r\n",
    "5. **Bootstrap Distribution:**\r\n",
    "   - Collect all the calculated mean heights to create the bootstrap distribution.\r\n",
    "\r\n",
    "6. **Confidence Interval:**\r\n",
    "   - Use the bootstrap distribution to construct the 95% confidence interval. The interval is defined by the 2.5th and 97.5th percentiles of the bootsat the true population mean height lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379dd68a-ff23-4793-96fd-52df0558fec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height: [14.23916553 15.33622558]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample data\n",
    "original_sample = np.random.normal(loc=15, scale=2, size=50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "B = 10000\n",
    "\n",
    "# Bootstrap sampling and calculation of mean heights\n",
    "bootstrap_means = [np.mean(np.random.choice(original_sample, size=len(original_sample), replace=True)) for _ in range(B)]\n",
    "\n",
    "# Confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval for Mean Height:\", confidence_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

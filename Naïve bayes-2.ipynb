{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a75803d-37c6-4835-b589-5e02e827578e",
   "metadata": {},
   "source": [
    "# Assignment - Na√Øve bayes-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa6d8c-e5c0-4e2c-bbbc-52d65df8489c",
   "metadata": {},
   "source": [
    "#### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?n?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57578384-d44c-4a8f-b55f-51599be4790c",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2b076-57e1-4536-aab0-e4976880471c",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use the conditional probability formula. Let's denote:\r\n",
    "\r\n",
    "- \\( A \\): the event that an employee uses the health insurance plan.\r\n",
    "- \\( B \\): the event that an employee is a smoker.\r\n",
    "\r\n",
    "The probability that an employee uses the health insurance plan is denoted by \\( P(A) \\), and the probability that an employee who uses the plan is a smoker is denoted by \\( P(B|A) \\).\r\n",
    "\r\n",
    "The conditional probability formula is given by:\r\n",
    "\r\n",
    "\\[ P(B|A) = \\frac{P(A \\cap B)}{P(A)} \\]\r\n",
    "\r\n",
    "Given that 70% of the employees use the health insurance plan (\\( P(A) = 0.7 \\)) and 40% of the employees who use the plan are smokers (\\( P(B|A) = 0.4 \\)), we can substitute these values into the formula:\r\n",
    "\r\n",
    "\\[ P(B|A) = \\frac{0.4 \\times 0.7}{0.7} = 0.4 \\]\r\n",
    "\r\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039d588-50b7-48c8-8c1a-bfa53aedc298",
   "metadata": {},
   "source": [
    "#### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25802504-c738-48e2-9b10-d05a451e8075",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05e141-792a-48a4-b3fd-8360f1838ec0",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the types of features they are designed to handle and the underlying assumptions about the distribution of these features.\r\n",
    "\r\n",
    "1. **Bernoulli Naive Bayes:**\r\n",
    "   - **Features:** Bernoulli Naive Bayes is designed for binary or boolean features, meaning each feature can take on one of two values (0 or 1).\r\n",
    "   - **Assumption:** It assumes that features are binary variables representing the presence (1) or absence (0) of certain characteristics.\r\n",
    "   - **Use Cases:**\r\n",
    "      - Document classification tasks where each term is either present (1) or absent (0).\r\n",
    "      - Any situation where features can be represented as binary values.\r\n",
    "\r\n",
    "   ```python\r\n",
    "   from sklearn.naive_bayes import BernoulliNB\r\n",
    "   classifier = BernoulliNB()\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Multinomial Naive Bayes:**\r\n",
    "   - **Features:** Multinomial Naive Bayes is designed for discrete features, typically representing counts or frequencies of events.\r\n",
    "   - **Assumption:** It assumes that features are multinomially distributed, meaning they represent counts of occurrences of different events.\r\n",
    "   - **Use Cases:**\r\n",
    "      - Text classification tasks where features are word frequencies or term frequencies.\r\n",
    "      - Any situation where features can be represented as counts of occurrences.\r\n",
    "\r\n",
    "   ```python\r\n",
    "   from sklearn.naive_bayes import MultinomialNB\r\n",
    "   classifier = MultinomialNB()\r\n",
    "   ```\r\n",
    "\r\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your features:\r\n",
    "\r\n",
    "- Use **Bernoulli Naive Bayes** if your features are binary (0 or 1).\r\n",
    "- Use **Multinomial Naive Bayes** if your features are discrete and represent counts or frequencies.\r\n",
    "\r\n",
    "Both classifiers follow the same principles of Naive Bayes, assuming independence between features given the class, and they are commonly used in natural language processing tasks such as text classification or spam filtering. The choice should align with the characteristics of your data and the specific requirements of your problem. inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221c46c-4c7a-44f0-a9cb-a64bd7c8f08a",
   "metadata": {},
   "source": [
    "#### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4c286-ca90-4c5e-b8ac-3dd7b72cc207",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bac4c-27cb-4583-918f-7febc0be2ca0",
   "metadata": {},
   "source": [
    "In scikit-learn's implementation of Bernoulli Naive Bayes, missing values are generally not handled explicitly. The algorithm assumes that the input data consists of binary features, where each feature is either present (1) or absent (0). If a feature has a missing value, it can be treated as if it is absent (0) during the classification process.\r\n",
    "\r\n",
    "Here are a few points to consider:\r\n",
    "\r\n",
    "1. **Binary Features:**\r\n",
    "   - Bernoulli Naive Bayes is designed for binary features, and it expects the input data to consist of binary values (0 or 1).\r\n",
    "   - If a feature has a missing value, it can be treated as 0, assuming the feature is absent.\r\n",
    "\r\n",
    "2. **Imputation:**\r\n",
    "   - If missing values are explicitly present in the dataset and you want to handle them, you may need to perform imputation before applying the Bernoulli Naive Bayes algorithm.\r\n",
    "   - Common imputation techniques include replacing missing values with the mode (most frequent value) or using more advanced imputation methods based on the characteristics of your data.\r\n",
    "\r\n",
    "3. **Preprocessing:**\r\n",
    "   - Ensure that the dataset is preprocessed and transformed into a suitable format for Bernoulli Naive Bayes, with binary features.\r\n",
    "\r\n",
    "Example of handling missing values:\r\n",
    "\r\n",
    "```python\r\n",
    "import pandas as pd\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.naive_bayes import BernoulliNB\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# Assume 'X' is your feature matrix and 'y' is the target variable\r\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "# Impute missing values (replace NaN with 0 for binary features)\r\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\r\n",
    "X_train_imputed = imputer.fit_transform(X_train)\r\n",
    "X_test_imputed = imputer.transform(X_test)\r\n",
    "\r\n",
    "# Create and train Bernoulli Naive Bayes classifier\r\n",
    "classifier = BernoulliNB()\r\n",
    "classifier.fit(X_train_imputed, y_train)\r\n",
    "\r\n",
    "# Make predictions\r\n",
    "y_pred = classifier.predict(X_test_imputed)\r\n",
    "```\r\n",
    "\r\n",
    "In this example, the `SimpleImputer` is used to replace missing values with 0, assuming that the missing value corresponds to the absence of the feature. It's important to choose an imputation strategy that aligns with the semantics of your data and the assumptions of the Bernoulli Naive Bayes algorithm.decisions.ed model complexity.m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506f36b-ec4b-4131-97bd-a2529494878f",
   "metadata": {},
   "source": [
    "#### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e31682-51b7-4ce7-98f0-5190cceb24fd",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9709b-71f6-446f-a76b-3f440ae34024",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that is suitable for continuous data, assuming that each class is characterized by a Gaussian (normal) distribution of the features.\r\n",
    "\r\n",
    "In the context of multi-class classification, Gaussian Naive Bayes models the likelihood of the features given the class using Gaussian distributions. The class with the highest posterior probability is then predicted for a given set of features.\r\n",
    "\r\n",
    "Here's how you can use Gaussian Naive Bayes for multi-class classification in scikit-learn:\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.datasets import load_iris\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.metrics import accuracy_score, classification_report\r\n",
    "\r\n",
    "# Load the Iris dataset as an example\r\n",
    "data = load_iris()\r\n",
    "X = data.data\r\n",
    "y = data.target\r\n",
    "\r\n",
    "# Split the data into training and testing sets\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "# Create and train Gaussian Naive Bayes classifier\r\n",
    "classifier = GaussianNB()\r\n",
    "classifier.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Make predictions on the test set\r\n",
    "y_pred = classifier.predict(X_test)\r\n",
    "\r\n",
    "# Evaluate the performance\r\n",
    "accuracy = accuracy_score(y_test, y_pred)\r\n",
    "report = classification_report(y_test, y_pred)\r\n",
    "\r\n",
    "print(f\"Accuracy: {accuracy}\")\r\n",
    "print(\"Classification Report:\\n\", report)\r\n",
    "```\r\n",
    "\r\n",
    "In this example, the Iris dataset is used for demonstration purposes, but you can apply the same approach to other datasets with continuous features. The `GaussianNB` class in scikit-learn automatically handles multi-class classification using the one-vs-all strategy.\r\n",
    "\r\n",
    "Remember that Gaussian Naive Bayes makes the assumption that the features within each class follow a Gaussian distribution. If this assumption aligns with the characteristics of your data, Gaussian Naive Bayes can be a simple and effective choice for multi-class classification problems.ke the SVM robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4b0aa-3542-43e7-9344-67e0dec5593b",
   "metadata": {},
   "source": [
    "#### Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cd730-89b7-4244-bc2f-d635a4a2e36e",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cdfef-78b0-49ab-8392-8022cc5fb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Spambase dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n",
    "    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
    "    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "    \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\",\n",
    "    \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\",\n",
    "    \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "    \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\",\n",
    "    \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\", \"spam\"\n",
    "]\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(\"spam\", axis=1)\n",
    "y = data[\"spam\"]\n",
    "\n",
    "# Implement Bernoulli Naive Bayes\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_scores = cross_val_score(bnb_classifier, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Implement Multinomial Naive Bayes\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_scores = cross_val_score(mnb_classifier, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Implement Gaussian Naive Bayes\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb_classifier, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Performance metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1 Score': f1_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "# Report metrics for each classifier\n",
    "bnb_metrics = calculate_metrics(y, cross_val_score(bnb_classifier, X, y, cv=10, scoring='accuracy'))\n",
    "mnb_metrics = calculate_metrics(y, cross_val_score(mnb_classifier, X, y, cv=10, scoring='accuracy'))\n",
    "gnb_metrics = calculate_metrics(y, cross_val_score(gnb_classifier, X, y, cv=10, scoring='accuracy'))\n",
    "\n",
    "print(\"Bernoulli Naive Bayes Metrics:\")\n",
    "print(bnb_metrics)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes Metrics:\")\n",
    "print(mnb_metrics)\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes Metrics:\")\n",
    "print(gnb_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
